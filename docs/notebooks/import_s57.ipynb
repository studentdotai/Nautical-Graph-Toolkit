{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Testing S-57 Data Processing Classes"
   ],
   "id": "b2324741c9e42a73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S-57 Data Conversion Workflows\n",
    "\n",
    "This notebook demonstrates comprehensive S-57 (Electronic Navigational Chart) conversion workflows\n",
    "using the `nautical_graph_toolkit` library. Learn how to convert raw S-57 files to multiple backend formats\n",
    "and verify data integrity across the conversion pipeline.\n",
    "\n",
    "## Conversion Modes\n",
    "\n",
    "### 1. **by_enc Mode** (S57Base)\n",
    "- Each S-57 file becomes a separate output database/schema\n",
    "- Useful for: Managing individual chart versions, isolated testing, source tracking\n",
    "- Output structure: One schema per ENC (e.g., `US1EEZ1M`, `US1GC09M`)\n",
    "\n",
    "### 2. **by_layer Mode** (S57Advanced) - RECOMMENDED\n",
    "- All S-57 files merged into a single database organized by object class (layer)\n",
    "- Useful for: Unified analysis, spatial queries, performance optimization\n",
    "- Features are stamped with `dsid_*` columns tracking source ENC, edition, and update\n",
    "- Better for: Routing, maritime analysis, data fusion\n",
    "\n",
    "## Backend Targets\n",
    "- **PostGIS**: Multi-user, server-based, enterprise-ready\n",
    "- **GeoPackage**: Portable single file, works offline\n",
    "- **SpatiaLite**: Self-contained SQLite, no external dependencies\n",
    "\n",
    "## Performance Considerations\n",
    "- S57Advanced with `auto_tune_batch_size=True`: Automatically optimizes memory and I/O\n",
    "- Parallel processing: Available for read-only file discovery (not database writes)\n",
    "- Large datasets (10000+ ENCs): May take 2-4 hours depending on hardware\n"
   ],
   "id": "import-intro-cell"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Notebook Structure and Workflow\n\nThis notebook is organized into the following steps:\n\n| Step | Name | Purpose | Duration |\n|------|------|---------|----------|\n| 1 | **Configuration** | Set backend, methods, and user parameters | 2 minutes |\n| 2 | **Imports** | Load all dependencies and set environment | < 1 minute |\n| 3 | **PostGIS Connection Test** | Verify database connectivity (if using PostGIS) | 1-2 seconds |\n| 4 | **S57Base Conversions** | Quick one-to-one conversion (5-30 minutes) | 5-30 minutes |\n| 5 | **S57Advanced Conversions** | Production layer-centric conversion (2-8 hours) | 2-8 hours |\n| 6 | **S57Updater** | Incremental updates to maintain databases | 10-30 minutes |\n| 7 | **DeepTest** | Comprehensive integration testing | 2-6 hours |\n\n### Simplified Configuration System\n\nInstead of managing 14 individual workflow flags, the new system uses:\n\n**Backend Selection:**\n```python\nbackend = 'postgis'  # Options: 'postgis', 'spatialite', 'gpkg'\n```\n\n**Conversion Methods:**\n```python\nmethods = {\n    'S57_Base': True,          # Run S57Base conversions\n    'S57_Advanced': True,      # Run S57Advanced conversions (RECOMMENDED)\n    'S57_Updater': False       # Run S57Updater incremental updates\n}\n```\n\n**Workflow Options:**\n```python\nworkflow_options = {\n    'run_parallel_processing': False,  # PostGIS only\n    'run_deeptest': False              # All backends\n}\n```\n\n### How to Use This Notebook\n\n**For Quick Testing:** \n```\nbackend = 'postgis'\nmethods = {'S57_Base': True, 'S57_Advanced': False, 'S57_Updater': False}\n```\nRun cells 1-4 (~5-30 minutes)\n\n**For Production Setup:** \n```\nbackend = 'postgis'\nmethods = {'S57_Base': True, 'S57_Advanced': True, 'S57_Updater': False}\n```\nRun cells 1-5 (~2-8 hours first time, includes validation)\n\n**For Updates:** \n```\nbackend = 'postgis'\nmethods = {'S57_Base': False, 'S57_Advanced': False, 'S57_Updater': True}\n```\nRun cells 1-3, then 6 (~10-30 minutes)\n\n**For Validation/Testing:** \n```\nbackend = 'postgis'  # or 'gpkg' or 'spatialite'\nworkflow_options = {'run_deeptest': True}\n```\nRun cells 1-3, then 7 (~2-6 hours)\n\n### Backend Notes\n\n- **PostGIS**: All features supported (conversions, updates, parallel processing)\n- **GeoPackage**: Conversions supported; updates not recommended (concurrent access issues)\n- **SpatiaLite**: Conversions supported; updates not recommended (concurrent access issues)\n\n### Important Pre-requisites\n\n**For PostGIS Backend:**\n- `.env` file must contain: `DB_NAME`, `DB_USER`, `DB_PASSWORD`, `DB_HOST`, `DB_PORT`\n- Connection test (Step 3) will verify credentials\n\n**Data Files:**\n- S-57 ENC files in `data_paths['s57_data_dir']` (provided in test dataset)\n- For S57Advanced, can optionally use larger dataset from `data_paths['active_enc']`\n\n**Disk Space Requirements (FIX 6):**\n- **S57Base conversions**: ~2-5x size of source .000 files\n- **S57Advanced conversions**: ~3-8x size of source data (varies by backend and dataset size)\n- **DeepTest**: Additional 2-3x for test outputs\n- **Recommended minimum**: 10GB free space for test datasets, 100GB+ for full NOAA ENC coverage\n- **SpatiaLite**: Generally smaller than GeoPackage, fastest conversion time (~1 hour for NOAA data)\n- **GeoPackage**: Moderate size, good for portable single-file deployments (~4 hours for NOAA data)\n- **PostGIS**: No local file size limits, distributed storage across database",
   "id": "ab62d046b76b218e"
  },
  {
   "cell_type": "markdown",
   "id": "el19x67e5em",
   "source": "## Step 1: Configuration\n\nAll user-configurable parameters are centralized below for easy modification. Adjust parameters in the next cell before running the notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fiy7txximq",
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK CONFIGURATION - Adjust these parameters before running\n",
    "# =============================================================================\n",
    "\n",
    "# --- Backend Selection (choose one) ---\n",
    "# Options: 'postgis', 'spatialite', 'gpkg'\n",
    "backend = 'gpkg'\n",
    "\n",
    "# --- Conversion Methods ---\n",
    "# Select which S-57 conversion methods to run\n",
    "methods = {\n",
    "    'S57_Base': True,       # Quick one-to-one conversion (5-30 minutes)\n",
    "    'S57_Advanced': True,   # Production layer-centric conversion (2-8 hours) - RECOMMENDED\n",
    "    'S57_Updater': True    # Incremental updates to existing database (10-30 minutes)\n",
    "}\n",
    "\n",
    "# --- Additional Workflow Options ---\n",
    "# PostGIS-only features and integration testing\n",
    "workflow_options = {\n",
    "    'run_parallel_processing': False,  # PostGIS only - test parallel file discovery\n",
    "    'run_deeptest': False              # Comprehensive integration testing (2-6 hours)\n",
    "}\n",
    "\n",
    "# --- Output Configuration ---\n",
    "# Base name for all S57Advanced and S57Updater outputs (schemas and filenames)\n",
    "# Change this single variable to rename all related outputs\n",
    "output_base_name = 'enc_west'\n",
    "\n",
    "# Derived output configuration (automatically generated from base name)\n",
    "output_config = {\n",
    "    # S57Base output (by_enc mode) - one database per ENC\n",
    "    'base_output_dir': 'by_enc',\n",
    "    \n",
    "    # S57Advanced configurations (all use output_base_name)\n",
    "    'pg_schema': output_base_name,\n",
    "    'gpkg_filename': f'{output_base_name}.gpkg',\n",
    "    'sqlite_filename': f'{output_base_name}.sqlite',\n",
    "    \n",
    "    # Parallel processing test configuration (separate schema for testing)\n",
    "    'parallel_schema': 's57_parallel_test',\n",
    "    \n",
    "    # S57Updater configuration (uses same base name)\n",
    "    'updater_schema': output_base_name,\n",
    "    'updater_encs': ['US3CA52M', 'US1GC09M'],  # Specific ENCs to update\n",
    "    'updater_gpkg': f'{output_base_name}.gpkg',\n",
    "    'updater_sqlite': f'{output_base_name}.sqlite',\n",
    "    \n",
    "    # DeepTest integration testing (separate schema for testing)\n",
    "    'deeptest_schema': 's57_deeptest',\n",
    "}\n",
    "\n",
    "# --- S57Updater Configuration ---\n",
    "# Control update behavior when methods['S57_Updater'] is enabled\n",
    "updater_config = {\n",
    "    'force_update': True,  # True: Replace ENCs entirely | False: Apply incremental updates\n",
    "}\n",
    "\n",
    "# --- DeepTest Configuration ---\n",
    "# Settings for comprehensive integration testing\n",
    "deeptest_config = {\n",
    "    'test_level': 3,                     # Test depth (1=minimal, 3=comprehensive)\n",
    "    'skip_postgis': False,               # Skip PostGIS tests\n",
    "    'skip_updates': False,               # Skip update testing\n",
    "    'cleanup_on_success': True,          # Clean up test artifacts after success\n",
    "    'clean_output': True,                # Clean output directory before test\n",
    "    'exclude_geometry_cols': [\"geometry\", \"geom\", \"wkb_geometry\"]  # Columns to exclude from output\n",
    "}\n",
    "\n",
    "# --- Configuration Summary ---\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ Configuration loaded successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸ“¦ Backend: {backend.upper()}\")\n",
    "print(f\"ðŸ“Š Methods:\")\n",
    "print(f\"   - S57_Base: {methods['S57_Base']}\")\n",
    "print(f\"   - S57_Advanced: {methods['S57_Advanced']}\")\n",
    "print(f\"   - S57_Updater: {methods['S57_Updater']}\")\n",
    "if backend == 'postgis':\n",
    "    print(f\"   - Parallel Processing: {workflow_options['run_parallel_processing']}\")\n",
    "print(f\"   - DeepTest: {workflow_options['run_deeptest']}\")\n",
    "if methods['S57_Updater']:\n",
    "    update_mode = \"Force update (replace ENCs)\" if updater_config['force_update'] else \"Normal update (incremental)\"\n",
    "    print(f\"\\nðŸ”„ S57Updater mode: {update_mode}\")\n",
    "print(f\"\\nðŸ“ Output base name: {output_base_name}\")\n",
    "print(f\"   - S57Advanced schema/file: {output_config['pg_schema']}\")\n",
    "print(f\"   - S57Updater schema/file: {output_config['updater_schema']}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nâš ï¸  Paths and database parameters will be set after STEP 2: IMPORTS\")\n",
    "print(\"   (These depend on project_root and environment variables)\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ps59ylmo26",
   "source": [
    "# =============================================================================\n",
    "# STEP 2: IMPORTS - Consolidated import statements for all notebook dependencies\n",
    "# =============================================================================\n",
    "\n",
    "# --- Standard Library ---\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Environment Setup ---\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Geospatial Libraries ---\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "\n",
    "# --- Jupyter/Display ---\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Environment Initialization ---\n",
    "# (These run during import phase to ensure configuration is available early)\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# --- S-57 Data Processing Core Classes ---\n",
    "from src.nautical_graph_toolkit.core.s57_data import (\n",
    "    S57Base,\n",
    "    S57Advanced,\n",
    "    S57Updater,\n",
    "    PostGISManager,\n",
    "    SpatiaLiteManager,\n",
    "    GPKGManager,\n",
    "    S57AdvancedConfig\n",
    ")\n",
    "\n",
    "# --- Database Connectivity ---\n",
    "from src.nautical_graph_toolkit.utils.db_utils import PostGISConnector\n",
    "\n",
    "# Load environment variables (.env file should be at project root)\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# --- Conditional Imports for DeepTest (only if needed) ---\n",
    "# DeepTest imports are loaded later in workflow_steps['run_deeptest'] section\n",
    "# This avoids importing test dependencies unless specifically requested\n",
    "\n",
    "print(\"âœ“ All imports loaded successfully!\")\n",
    "print(f\"âœ“ Python path includes: {project_root}\")\n",
    "print(f\"âœ“ GDAL version: {gdal.__version__}\")\n",
    "print(f\"âœ“ GeoPandas version: {gpd.__version__}\")"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fuevktceoq7",
   "source": "## Step 2.5: Data Paths and Database Setup\n\nNow that imports are complete, set up data paths and database connection parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tj6dqtje6zc",
   "source": "# --- Data Source Paths ---\n# Define locations for input ENC data and output databases\ndata_paths = {\n    # Primary ENC data directory (used for S57Advanced large-scale conversions)\n    # Contains complete NOAA ENC dataset for comprehensive testing\n    'active_enc': project_root / 'data' / 'ENC_SF_LA' / 'ENC_ROOT',\n    \n    # Test/sample ENC data directory (used for S57Base and smaller tests)\n    # Contains subset of ENCs for quick conversions and verification\n    's57_data_dir': project_root / 'data' / 'ENC_ROOT',\n    \n    # ENC update files (used by S57Updater for incremental updates)\n    # Contains newer chart updates to test update mechanisms\n    's57_data_update_dir': project_root / 'data' / 'ENC_ROOT_UPDATE',\n    \n    # Output directory for all conversion results\n    'output_dir': Path.cwd() / 'output',\n    \n    # Test output directory (used by DeepTest integration testing)\n    'test_output_dir': project_root / 'test_output'\n}\n\n# FIX 5: Validate critical paths exist before proceeding\n# This prevents silent failures if data directories are missing\nprint(\"Validating data paths...\")\nrequired_paths = {\n    's57_data_dir': data_paths['s57_data_dir'],\n    'output_dir': data_paths['output_dir']\n}\n\nfor path_name, path in required_paths.items():\n    if path.exists():\n        print(f\"  âœ“ {path_name}: {path}\")\n    else:\n        print(f\"  âš ï¸  {path_name} does not exist: {path}\")\n        print(f\"     Create this directory or adjust data_paths configuration.\")\n\n# --- Database Connection Parameters ---\n# PostGIS connection details (from .env file)\n# Only used when backend = 'postgis'\ndb_params = {\n    'dbname': os.getenv('DB_NAME'),\n    'user': os.getenv('DB_USER'),\n    'password': os.getenv('DB_PASSWORD'),\n    'host': os.getenv('DB_HOST'),\n    'port': os.getenv('DB_PORT')\n}\n\n# --- S57Advanced Configuration Objects ---\n# Define conversion behavior for different scenarios\ns57_advanced_config = {\n    # Default configuration: auto-tuned batch processing\n    # Recommended for most conversions - balances speed and memory\n    'default': S57AdvancedConfig(\n        auto_tune_batch_size=True,\n        enable_debug_logging=False\n    ),\n\n    # Parallel processing configuration: high-safety read-only approach\n    # Safe for production - only parallelizes file discovery, not database writes\n    'parallel': S57AdvancedConfig(\n        enable_parallel_processing=True,    # Use multiple cores\n        parallel_read_only=True,            # Read-only operations only\n        parallel_db_writes=False,           # Writes are sequential\n        parallel_validation_level='strict', # Maximum data quality checks\n        max_parallel_workers=2,             # Conservative worker count for stability\n        enable_debug_logging=False          # Disable debug logging during config creation\n    )\n}\n\n# --- Summary ---\nprint(\"=\" * 70)\nprint(\"âœ“ Paths and Database Parameters Configured!\")\nprint(\"=\" * 70)\nprint(f\"\\nðŸ“ Data directories:\")\nprint(f\"   - Active ENC: {data_paths['active_enc']}\")\nprint(f\"   - Test ENC:   {data_paths['s57_data_dir']}\")\nprint(f\"   - Output:     {data_paths['output_dir']}\")\nif backend == 'postgis':\n    print(f\"\\nðŸ—„ï¸  PostGIS Connection:\")\n    print(f\"   - Database: {db_params.get('dbname', 'NOT CONFIGURED')}\")\n    print(f\"   - Host:     {db_params.get('host', 'NOT CONFIGURED')}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# =============================================================================\n# STEP 3: PostGIS Connection Test\n# =============================================================================\n# IMPORTANT: Test the database connection early for PostGIS backend\n# This ensures all PostGIS operations will succeed before proceeding.\n\nif backend == 'postgis':\n    print(\"=\" * 70)\n    print(\"Testing PostGIS database connection...\")\n    print(\"=\" * 70)\n    try:\n        pg = PostGISConnector(db_params)\n        pg.connect()\n        schemas = pg.get_schemas()\n        print(f\"âœ“ PostGIS connection successful!\")\n        print(f\"âœ“ Available schemas: {schemas}\")\n        print(\"=\" * 70)\n    except Exception as e:\n        print(f\"âœ— PostGIS connection failed: {e}\")\n        print(\"  Please check your .env file and database credentials before proceeding.\")\n        print(\"=\" * 70)\nelse:\n    print(\"=\" * 70)\n    print(f\"PostGIS connection test skipped (backend = '{backend}', not 'postgis')\")\n    print(\"=\" * 70)",
   "id": "1829ebffc988c2f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Check GDAL version (always runs - no flag to skip)\nprint(\"=\" * 70)\nprint(\"GDAL and Library Versions:\")\nprint(\"=\" * 70)\nprint(f\"GDAL Python bindings version: {gdal.__version__}\")\nprint(f\"GDAL C library version: {gdal.VersionInfo('RELEASE_NAME')}\")\nprint(f\"GeoPandas version: {gpd.__version__}\")\nprint(f\"Fiona version: {fiona.__version__}\")\nprint(\"=\" * 70)",
   "id": "a0df839b3262c5e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4: S57Base - Simple Bulk Conversion (by_enc mode)\n\n`S57Base` is the simplest conversion class, creating one output per input S-57 file.\n\n### Key Characteristics\n\n| Feature | Details |\n|---------|---------|\n| **Processing Mode** | One S-57 file â†’ One output database/schema |\n| **Tool Used** | GDAL's `VectorTranslate` |\n| **Feature Stamping** | None - raw ENC data only |\n| **Speed** | Fast - minutes to hours depending on ENC count |\n| **Output Locations** | Multiple backends: GeoPackage, PostGIS, SpatiaLite |\n\n### Use Cases\n- Initial ENC assessment and validation\n- Isolated testing of specific charts\n- Preserving individual ENC versions unchanged\n- Quick conversions of small datasets\n- Debugging specific ENC problems\n\n### What Happens During Conversion\n1. Scans input directory for `.000` files (S-57 base files)\n2. Uses GDAL's `VectorTranslate` for one-to-one mapping\n3. Creates separate output per chart\n4. Preserves original S-57 object class organization\n\n### Limitation\nNo feature stamping or cross-chart analysis capability - see S57Advanced for production use.",
   "id": "37300362ce430416"
  },
  {
   "cell_type": "markdown",
   "id": "4b65k68b2ue",
   "source": "## Comparison: S57Base vs S57Advanced\n\n| Aspect | S57Base (by_enc) | S57Advanced (by_layer) |\n|--------|------------------|----------------------|\n| **Output Structure** | One database per ENC file | Single unified database with all ENCs |\n| **Organization** | By source chart | By S-57 object class (layer) |\n| **Feature Stamping** | None | Yes: `dsid_dsnm`, `dsid_edtn`, `dsid_updn` |\n| **Cross-chart Analysis** | Limited | Excellent - all layers contain all relevant features |\n| **Conversion Speed** | Fast (minutes) | Slow (hours) - but only needed once |\n| **Best For** | Testing, QA, isolated analysis | Production integration, spatial queries, routing |\n| **Data Duplication** | High - object classes repeated in each ENC | None - normalized by layer |\n| **Spatial Queries** | Must query multiple databases | Single database query |\n\n### When to Use Each Mode\n\n**Use S57Base if you need to:**\n- Quickly test a few ENC files\n- Preserve original ENC boundaries and organization\n- Work with individual chart editions\n- Perform isolated validation checks\n\n**Use S57Advanced if you need to:**\n- Build a production maritime database\n- Run spatial analysis across chart boundaries\n- Support routing and pathfinding algorithms\n- Track data provenance with `dsid_*` columns\n- Minimize storage overhead",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Convert to GPKG"
   ],
   "id": "8bf0262726de8ea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# S57Base conversion to selected backend\nif methods['S57_Base'] and backend == 'gpkg':\n    # import logging\n    # logging.getLogger('src.nautical_graph_toolkit.core.s57_data').setLevel(logging.DEBUG)\n\n    base_converter = S57Base(\n            input_path=data_paths['s57_data_dir'],\n            output_dest=str(data_paths['output_dir'] / 'by_enc'),\n            output_format='gpkg',\n            overwrite=True\n        )\n    base_converter.convert_by_enc()\nelif methods['S57_Base']:\n    print(f\"S57Base GeoPackage conversion skipped (backend = '{backend}', not 'gpkg')\")\nelse:\n    print(\"S57Base conversion skipped (methods['S57_Base'] = False)\")",
   "id": "e7091bcbb2ff4ae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Convert to PostGIS"
   ],
   "id": "19e6f973ebe3cfc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# S57Base conversion to PostGIS\nif methods['S57_Base'] and backend == 'postgis':\n    base_converter = S57Base(\n            input_path=data_paths['s57_data_dir'],\n            output_dest=db_params,\n            output_format='postgis',\n            overwrite=False\n        )\n    base_converter.convert_by_enc()\nelif methods['S57_Base']:\n    print(f\"S57Base PostGIS conversion skipped (backend = '{backend}', not 'postgis')\")\nelse:\n    print(\"S57Base conversion skipped (methods['S57_Base'] = False)\")",
   "id": "9341dd7e7bac52b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Verify PostGIS schema after S57Base conversion\nif methods['S57_Base'] and backend == 'postgis':\n    pg = PostGISConnector(db_params)\n    pg.connect()\n    pg.get_schema_summary()\nelif methods['S57_Base']:\n    print(f\"PostGIS schema verification skipped (backend = '{backend}', not 'postgis')\")\nelse:\n    print(\"PostGIS schema verification skipped (S57Base conversion not enabled)\")",
   "id": "b797296606404c55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Convert to SpatiaLite"
   ],
   "id": "979d989d5e3c871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# S57Base conversion to SpatiaLite\nif methods['S57_Base'] and backend == 'spatialite':\n    base_converter = S57Base(\n            input_path=data_paths['s57_data_dir'],\n             output_dest=str(data_paths['output_dir'] / 'by_enc'),\n            output_format='spatialite',\n            overwrite=True\n        )\n    base_converter.convert_by_enc()\nelif methods['S57_Base']:\n    print(f\"S57Base SpatiaLite conversion skipped (backend = '{backend}', not 'spatialite')\")\nelse:\n    print(\"S57Base conversion skipped (methods['S57_Base'] = False)\")",
   "id": "2f78e079437eac50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 5: S57Advanced - Optimized Layer-Centric Conversion (by_layer mode) - RECOMMENDED\n\n`S57Advanced` performs optimized, layer-centric conversion merging all inputs into a unified database. **This is the recommended approach for production maritime databases.**\n\n### Key Features\n\n| Feature | Benefit |\n|---------|---------|\n| **Layer-centric organization** | All data organized by S-57 object class (e.g., all soundings together) - enables powerful spatial queries |\n| **Feature stamping** | Each feature tagged with source ENC (`dsid_dsnm`), edition (`dsid_edtn`), update (`dsid_updn`) - full data lineage |\n| **Batch processing** | Optimized memory management with `auto_tune_batch_size` - handles 10000+ ENCs |\n| **Parallel processing** | Optional multi-threaded file discovery (read-only) - faster preprocessing |\n| **Auto-tuning** | Automatically adjusts batch size based on available memory - no manual tuning needed |\n\n### Use Cases\n- **Production data integration**: Build comprehensive maritime databases supporting multi-user access\n- **Spatial analysis**: Query all features of a type across all charts (e.g., \"all lights in US waters\")\n- **Routing and pathfinding**: Need unified navigable area representation for navigation algorithms\n- **Data fusion**: Combine S-57 with other maritime data sources\n- **Historical tracking**: Use `dsid_edtn` and `dsid_updn` columns to analyze chart evolution\n\n### Configuration Options\n- `auto_tune_batch_size`: Let system optimize performance automatically (recommended)\n- `enable_parallel_processing`: Use multiple cores for file discovery\n- `parallel_validation_level`: Balance speed vs. data quality checks ('strict' is recommended)\n\n### Why Layer-Centric is Better for Production\n1. **Single Query**: One SQL query gets all relevant features instead of querying 100+ databases\n2. **Data Integrity**: Feature stamping ensures traceability of data sources\n3. **Performance**: Spatial indexes optimized for layer-based queries\n4. **Consistency**: All features in a layer have uniform attributes and structure",
   "id": "df519470cdfbea66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# S57Advanced conversion to PostGIS\nif methods['S57_Advanced'] and backend == 'postgis':\n    # --- Step 1: Conversion ---\n    # NOAA ENC conversion - 2h 24m\n    pg_schema = output_config['pg_schema']\n    print(f\"--- Starting S57Advanced conversion to PostGIS schema: '{pg_schema}' ---\")\n\n    advanced_converter_pg = S57Advanced(\n        input_path=data_paths['active_enc'],\n        output_dest=db_params,\n        output_format='postgis',\n        overwrite=True,\n        schema=pg_schema,\n        config=s57_advanced_config['default']\n    )\n    advanced_converter_pg.convert_to_layers()\n\n    print(\"\\n--- Step 2: Verification ---\")\n    # --- Step 2: Verification ---\n    try:\n        manager = PostGISManager(db_params=db_params, schema=pg_schema)\n\n        # Verify a common layer exists and has data\n        depare_layer = manager.get_layer('lndmrk')\n        print(f\"Loaded {len(depare_layer)} features from LNDMRK layer.\")\n        print(\"LNDMRK layer head (note the 'dsid_*' stamping columns):\")\n        display(depare_layer.head())\n\n        # Verify feature stamping integrity across all layers\n        print(\"\\nVerifying feature update status against DSID table...\")\n        verification_results = manager.verify_feature_update_status()\n        print(\"Verification Summary:\")\n        display(verification_results)\n\n    except Exception as e:\n        print(f\"An error occurred during PostGIS verification: {e}\")\nelif methods['S57_Advanced']:\n    print(f\"S57Advanced PostGIS conversion skipped (backend = '{backend}', not 'postgis')\")\nelse:\n    print(\"S57Advanced conversion skipped (methods['S57_Advanced'] = False)\")",
   "id": "bb46dc5ad43b2bf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2. Convert to GeoPackage and Verify\n",
    "\n",
    "Next, we test the conversion to a single GeoPackage file. The `GPKGManager` is used for verification."
   ],
   "id": "10cf790ebefaed57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# S57Advanced conversion to GeoPackage\nif methods['S57_Advanced'] and backend == 'gpkg':\n    # --- Step 1: Conversion ---\n    # NOAA ENC conversion - 4h 9m\n    gpkg_path = data_paths['output_dir'] / output_config['gpkg_filename']\n    print(f\"--- Starting S57Advanced conversion to GeoPackage: '{gpkg_path}' ---\")\n\n    advanced_converter_gpkg = S57Advanced(\n        input_path=data_paths['active_enc'],\n        output_dest=str(gpkg_path),\n        output_format='gpkg',\n        overwrite=True,\n        config=s57_advanced_config['default']\n    )\n    advanced_converter_gpkg.convert_to_layers()\n\n    print(\"\\n--- Step 2: Verification ---\")\n    # --- Step 2: Verification ---\n    try:\n        manager = GPKGManager(gpkg_path=gpkg_path)\n\n        # Verify a common layer exists and has data\n        soundg_layer = manager.get_layer('lndmrk')\n        print(f\"Loaded {len(soundg_layer)} features from LNDMRK layer in the GeoPackage.\")\n        print(\"LNDMRK layer head:\")\n        display(soundg_layer.head())\n\n        # Verify feature stamping integrity\n        print(\"\\nVerifying feature update status...\")\n        verification_results = manager.verify_feature_update_status()\n        print(\"Verification Summary:\")\n        display(verification_results)\n\n    except Exception as e:\n        print(f\"An error occurred during GeoPackage verification: {e}\")\nelif methods['S57_Advanced']:\n    print(f\"S57Advanced GeoPackage conversion skipped (backend = '{backend}', not 'gpkg')\")\nelse:\n    print(\"S57Advanced conversion skipped (methods['S57_Advanced'] = False)\")",
   "id": "b6d187d55b16ea65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3. Convert to SpatiaLite and Verify\n",
    "\n",
    "Finally, we test the conversion to a SpatiaLite database file and verify it with `SpatiaLiteManager`."
   ],
   "id": "905141137d20d2c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# S57Advanced conversion to SpatiaLite\nif methods['S57_Advanced'] and backend == 'spatialite':\n    # --- Step 1: Conversion ---\n    # NOAA ENC conversion - 1h 1m\n    sqlite_path = data_paths['output_dir'] / output_config['sqlite_filename']\n    print(f\"--- Starting S57Advanced conversion to SpatiaLite: '{sqlite_path}' ---\")\n\n    advanced_converter_sqlite = S57Advanced(\n        input_path=data_paths['active_enc'],\n        output_dest=str(sqlite_path),\n        output_format='spatialite',\n        overwrite=True,\n        config=s57_advanced_config['default']\n    )\n    advanced_converter_sqlite.convert_to_layers()\n\n    print(\"\\n--- Step 2: Verification ---\")\n    # --- Step 2: Verification ---\n    try:\n        manager = SpatiaLiteManager(db_path=sqlite_path)\n\n        # Verify a common layer exists and has data\n        boyspp_layer = manager.get_layer('boyspp')\n        print(f\"Loaded {len(boyspp_layer)} features from BOYSPP layer in the SpatiaLite DB.\")\n        print(\"BOYSPP layer head:\")\n        display(boyspp_layer.head())\n\n        # Verify feature stamping integrity\n        print(\"\\nVerifying feature update status...\")\n        verification_results = manager.verify_feature_update_status()\n        print(\"Verification Summary:\")\n        display(verification_results)\n\n    except Exception as e:\n        print(f\"An error occurred during SpatiaLite verification: {e}\")\nelif methods['S57_Advanced']:\n    print(f\"S57Advanced SpatiaLite conversion skipped (backend = '{backend}', not 'spatialite')\")\nelse:\n    print(\"S57Advanced conversion skipped (methods['S57_Advanced'] = False)\")",
   "id": "3ec0591c4a914056",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4. Test with Enterprise-Safe Parallel Processing\n",
    "\n",
    "This test demonstrates using the `S57AdvancedConfig` to enable high-safety, read-only parallel processing, which can speed up the initial file discovery and preprocessing steps."
   ],
   "id": "cedc842f63838aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Test S57Advanced with Enterprise-Safe Parallel Processing (PostGIS only)\nif workflow_options['run_parallel_processing'] and backend == 'postgis':\n    # --- Step 1: Configure and Convert ---\n    parallel_schema = output_config['parallel_schema']\n    print(f\"--- Starting S57Advanced conversion with PARALLEL processing to schema: '{parallel_schema}' ---\")\n\n    # Use the recommended high-safety configuration for parallel reads\n    high_safety_config = s57_advanced_config['parallel']\n\n    print(\"\\nUsing configuration:\")\n    print(high_safety_config.get_configuration_summary())\n\n    advanced_converter_parallel = S57Advanced(\n        input_path=data_paths['s57_data_dir'],\n        output_dest=db_params,\n        output_format='postgis',\n        overwrite=True,\n        schema=parallel_schema,\n        config=high_safety_config\n    )\n    advanced_converter_parallel.convert_to_layers()\n\n    print(\"\\n--- Step 2: Verification ---\")\n    # --- Step 2: Verification ---\n    try:\n        manager = PostGISManager(db_params=db_params, schema=parallel_schema)\n        summary = manager.get_enc_summary()\n        print(f\"Successfully created {len(summary)} ENCs in the parallel-processed schema.\")\n        display(summary.head())\n    except Exception as e:\n        print(f\"An error occurred during parallel processing verification: {e}\")\nelif workflow_options['run_parallel_processing']:\n    print(f\"Parallel processing test skipped (backend = '{backend}', not 'postgis')\")\n    print(\"Parallel processing is only available with PostGIS backend.\")\nelse:\n    print(\"Parallel processing test skipped (workflow_options['run_parallel_processing'] = False)\")",
   "id": "f40a28b037d1e4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 6: S57Updater - Incremental Updates\n\n`S57Updater` handles incremental updates to an existing S57Advanced database, maintaining a production ENC database with latest chart updates.\n\n### Update Modes\n\n| Mode | Method | Use Case | Configuration |\n|------|--------|----------|---|\n| **Force Update** | `force_update_from_location()` | Replace specific ENCs entirely (e.g., after fixing errors) | `updater_config['force_update'] = True` |\n| **Normal Update** | `update_from_location()` | Apply incremental updates if available from NOAA | `updater_config['force_update'] = False` |\n\n**Configure which mode to use in Step 1:**\n```python\nupdater_config = {\n    'force_update': True,  # True: Replace ENCs entirely | False: Apply incremental updates\n}\n```\n\n### How It Works\n1. Compares local ENC versions with update files or source data\n2. Re-converts and replaces features **only for changed ENCs** (fast!)\n3. Maintains transaction integrity - all changes succeed or all fail (ACID)\n4. Provides detailed change summaries showing before/after versions\n\n### Change Summary Information\nAfter an update, you can see:\n- Old Edition â†’ New Edition for each updated ENC\n- Update number changes tracking version history\n- All changes recorded in database metadata tables\n\n### Backend Support\n\n| Backend | Status | Notes |\n|---------|--------|-------|\n| **PostGIS** | âœ… Fully Supported | Transactional updates with full ACID guarantees |\n| **SpatiaLite** | âš ï¸ Use with Care | Works but can cause \"database disk image is malformed\" errors with concurrent access |\n| **GeoPackage** | âš ï¸ Use with Care | Similar issues to SpatiaLite with concurrent access |\n\n### Important Caution\nOnly use with PostGIS for production systems. File-based updates (SpatiaLite/GeoPackage) can cause corruption if the file is accessed by multiple uncoordinated connections simultaneously. **PostGIS is the only backend recommended for update operations in production.**",
   "id": "4f534b747de2a096"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example Update Summary Output (FIX 8)\n\nWhen running S57Updater with `force_update_from_location()`, you'll see output like this showing edition and update number changes for each ENC:\n\n| Old Edition | New Edition |\n|--|--|\n| US1EEZ1M 9:0 | 9:0 |\n| US1GC09M 65:5 | 71:1 |\n| US1PO02M 21:1 | 21:1 |\n| US2WC12M 27:8 | 27:12 |\n| US3CA52M 31:0 | 31:7 |\n| US4CA60M 37:6 | 38:2 |\n\nThe format is `ENC_NAME edition:update`. This example shows several ENCs were updated to newer editions/updates, while others (US1EEZ1M, US1PO02M) were already current.",
   "id": "7f9aae73dbb66110"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PostGIS Updater"
   ],
   "id": "82bbc06dd6b81dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Test S57Updater with selected backend\nif methods['S57_Updater'] and backend == 'postgis':\n    db_schema = output_config['updater_schema']\n    updater = S57Updater(output_format='postgis',\n                         dest_conn=db_params,\n                         schema=db_schema)\n    \n    # Execute selected update mode based on configuration\n    if updater_config['force_update']:\n        print(\"Running force update (replacing specific ENCs)...\")\n        updater.force_update_from_location(\n            data_paths['s57_data_dir'], \n            enc_filter=output_config['updater_encs']\n        )\n    else:\n        print(\"Running normal incremental update...\")\n        updater.update_from_location(data_paths['s57_data_update_dir'])\n    \n    summary_df = updater.get_change_summary()\n    display(summary_df)\nelif methods['S57_Updater']:\n    print(f\"S57Updater PostGIS test skipped (backend = '{backend}', not 'postgis')\")\nelse:\n    print(\"S57Updater test skipped (methods['S57_Updater'] = False)\")",
   "id": "73a5b5d183e2c059",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2. SpatiaLite Updater\n",
    "\n",
    "This test demonstrates the `S57Updater` functionality for a SpatiaLite database. The previous implementation failed with a `database disk image is malformed` error because it attempted to run the updater on a file that was being accessed by multiple, uncoordinated connections (OGR and SQLAlchemy).\n",
    "\n",
    "The corrected test follows a robust, isolated process:\n",
    "\n",
    "1.  **Create a fresh SpatiaLite database** from the initial ENC data (`s57_data_dir`). This provides a clean, known state to update and prevents file corruption.\n",
    "2.  **Run the updater** on this new database using the update data (`s57_data_update_dir`).\n",
    "3.  **Verify the results** by comparing the ENC versions before and after the update and checking for data integrity."
   ],
   "id": "c5282e137646ad72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Test S57Updater with SpatiaLite\nif methods['S57_Updater'] and backend == 'spatialite':\n    print(\"âš ï¸  WARNING: SpatiaLite updates can cause 'database disk image is malformed' errors\")\n    print(\"   PostGIS is recommended for production update operations.\")\n    print()\n    \n    sqlite_update_target_path = data_paths['output_dir'] / output_config['updater_sqlite']\n    updater = S57Updater(\n         output_format='spatialite',\n         dest_conn=str(sqlite_update_target_path),\n         schema='main' # SpatiaLite doesn't use schemas like PostGIS, but 'main' is the default\n    )\n    \n    # Execute selected update mode based on configuration\n    if updater_config['force_update']:\n        print(\"Running force update (replacing specific ENCs)...\")\n        updater.force_update_from_location(\n            data_paths['s57_data_dir'], \n            enc_filter=output_config['updater_encs']\n        )\n    else:\n        print(\"Running normal incremental update...\")\n        updater.update_from_location(data_paths['s57_data_update_dir'])\n    \n    summary_df = updater.get_change_summary()\nelif methods['S57_Updater']:\n    print(f\"S57Updater SpatiaLite test skipped (backend = '{backend}', not 'spatialite')\")\nelse:\n    print(\"S57Updater test skipped (methods['S57_Updater'] = False)\")",
   "id": "8e831881b3ebf615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GPKG Updater"
   ],
   "id": "4d3e9af2f5e1cc0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Test S57Updater with GeoPackage\nif methods['S57_Updater'] and backend == 'gpkg':\n    print(\"âš ï¸  WARNING: GeoPackage updates can cause corruption with concurrent access\")\n    print(\"   PostGIS is recommended for production update operations.\")\n    print()\n    \n    update_target_path = data_paths['output_dir'] / output_config['updater_gpkg']\n    updater = S57Updater(\n         output_format='gpkg',\n         dest_conn=str(update_target_path),\n         schema='main' # GeoPackage doesn't use schemas like PostGIS, but 'main' is the default\n    )\n    \n    # Execute selected update mode based on configuration\n    if updater_config['force_update']:\n        print(\"Running force update (replacing specific ENCs)...\")\n        updater.force_update_from_location(\n            data_paths['s57_data_dir'], \n            enc_filter=output_config['updater_encs']\n        )\n    else:\n        print(\"Running normal incremental update...\")\n        updater.update_from_location(data_paths['s57_data_update_dir'])\n    \n    summary_df = updater.get_change_summary()\nelif methods['S57_Updater']:\n    print(f\"S57Updater GeoPackage test skipped (backend = '{backend}', not 'gpkg')\")\nelse:\n    print(\"S57Updater test skipped (methods['S57_Updater'] = False)\")",
   "id": "7f1fb2ab4bcd29af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 7: DeepTest - Comprehensive Integration Testing\n\nDeepTest is a comprehensive integration testing framework that exercises the complete S-57 conversion pipeline across all backends and scenarios.\n\n### What DeepTest Tests\n\n| Component | Tests | Duration |\n|-----------|-------|----------|\n| **S57Base Conversions** | All three backends (PostGIS, GeoPackage, SpatiaLite) | ~1 hour |\n| **S57Advanced Conversions** | All three backends with full dataset | ~2-8 hours |\n| **Update Mechanisms** | Apply updates to each backend independently | ~30 minutes |\n| **Data Integrity** | Feature counts, geometry validity, dsid stamping | Continuous |\n| **Multi-user Access** | Concurrent database connections | Depends on backend |\n\n### Test Levels\n\n| Level | Scope | Duration | When to Use |\n|-------|-------|----------|------------|\n| **1 (Minimal)** | Quick smoke test on small ENC subset | ~15 minutes | CI/CD pipelines, quick validation |\n| **2 (Standard)** | Full S57Base and S57Advanced conversions | ~2 hours | Regular testing, code validation |\n| **3 (Comprehensive)** | All conversions + updates + multi-user testing | ~6 hours | Release testing, certification |\n\n### Configuration Options in DeepTest\n\n| Option | Purpose | Default |\n|--------|---------|---------|\n| `test_level` | How thorough the test should be (1-3) | 3 |\n| `skip_postgis` | Skip PostGIS tests | False |\n| `skip_updates` | Skip update mechanism tests | False |\n| `cleanup_on_success` | Delete test artifacts after successful run | True |\n| `clean_output` | Clear output directory before starting | True |\n\n### Why Run DeepTest\n1. **Verify Conversion Pipeline**: Ensure all backends work correctly\n2. **Regression Testing**: Catch breaking changes in library updates\n3. **Data Quality**: Validate that converted data maintains integrity\n4. **Performance Baseline**: Track conversion times across versions\n5. **Certification**: Prove system ready for production deployment",
   "id": "9eaaf406a29e4bf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setup"
   ],
   "id": "13943c58c858536a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Conditional DeepTest Imports ---\n",
    "# Only import if we plan to run DeepTest (conditional import)\n",
    "if workflow_options.get('run_deeptest', False):\n",
    "    from tests.core__real_data.deep_test_s57_workflow import *\n",
    "else:\n",
    "    print(\"DeepTest imports skipped (workflow_options['run_deeptest'] = False)\")\n",
    "    print(\"To enable DeepTest, set workflow_options['run_deeptest'] = True in STEP 1 configuration\")\n",
    "\n",
    "if workflow_options.get('run_deeptest', False):\n",
    "    # --- DeepTest Configuration (used only if run_deeptest=True) ---\n",
    "    db_params_test = {\n",
    "        'dbname': os.getenv('DB_NAME'),\n",
    "        'user': os.getenv('DB_USER'),\n",
    "        'password': os.getenv('DB_PASSWORD'),\n",
    "        'host': os.getenv('DB_HOST'),\n",
    "        'port': os.getenv('DB_PORT')\n",
    "    }\n",
    "    db_schema = 's57_deeptest'\n",
    "    test_output_path = project_root / 'test_output'\n",
    "\n",
    "    test_config = TestConfig(s57_data_root = data_paths['s57_data_dir'],\n",
    "                             s57_update_root=data_paths['s57_data_update_dir'],\n",
    "                             test_output_dir = test_output_path,\n",
    "                             test_level = deeptest_config['test_level'],\n",
    "\n",
    "                             skip_postgis = deeptest_config['skip_postgis'],\n",
    "                             skip_updates = deeptest_config['skip_updates'],\n",
    "                             cleanup_on_success = deeptest_config['cleanup_on_success'],\n",
    "                             postgis_config = db_params_test,\n",
    "                             test_schema_name = db_schema,\n",
    "                             clean_output = deeptest_config['clean_output'],\n",
    "                             exclude_extra_cols= deeptest_config['exclude_geometry_cols'],\n",
    "    )\n",
    "\n",
    "\n",
    "    if workflow_options.get('run_deeptest', False):\n",
    "        print(f\"\\n--- Starting S57 Deep Test process with TEST CONFIG ---\")\n",
    "        print(f\"\\n--- Setting up directories:\")\n",
    "        print(f\"\\nTest dataset at:       {test_config.s57_data_root}\")\n",
    "        print(f\"\\nUpdate dataset at:     {test_config.s57_update_root}\")\n",
    "        print(f\"\\nTest output directory: {test_config.test_output_dir}\")\n",
    "        print(f\"\\n-----------------------------------\")\n",
    "        post_conf = (f\"Database: {db_params_test['dbname']} | Schema/Filename: {db_schema}\") if  test_config.skip_postgis == False else \"\"\n",
    "        print(f\"\\nPostGIS tests:   {'âŒ' if test_config.skip_postgis == True else 'âœ…'} {post_conf}\")\n",
    "        print(f\"\\nUpdates process: {'âŒ' if test_config.skip_updates == True else 'âœ…'}\")"
   ],
   "id": "a4932625147fca15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load Dataset"
   ],
   "id": "e0ba23a0adf692e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Execute DeepTest\nif workflow_options.get('run_deeptest', False):\n    try:\n        print(f\"\\nðŸŽ‰ Commence DeepTest!\")\n        tester = S57DeepTester(test_config)\n    except Exception as e:\n        print(f\"DeepTest execution failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\n    # Testing Update Readiness\n    if test_config.skip_updates == False:\n        compare_df = tester.analyze_update_readiness()\n        display(compare_df)\nelse:\n    print(\"DeepTest execution skipped (workflow_options['run_deeptest'] = False)\")\n    print(\"To run DeepTest, set workflow_options['run_deeptest'] = True in STEP 1 configuration\")",
   "id": "6d7cbd3f8e8ecce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Run comprehensive DeepTest\nif workflow_options.get('run_deeptest', False):\n    try:\n        report = tester.run_comprehensive_test()\n\n        print(f\"\\nðŸŽ‰ DeepTest completed successfully!\")\n        print(f\"ðŸ“ Results saved to: {test_config.test_output_dir}\")\n\n    except Exception as e:\n        print(f\"DeepTest execution failed: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"Comprehensive DeepTest skipped (workflow_options['run_deeptest'] = False)\")",
   "id": "85668c653592f134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# =============================================================================\n# POST-DEEPTEST VERIFICATION AND DATA EXPLORATION\n# =============================================================================\n# This cell validates the DeepTest workflow output by:\n# 1. Opening the test-generated database (GeoPackage or SpatiaLite)\n# 2. Enumerating all S-57 layers that were successfully converted\n# 3. Reading and inspecting a specific layer (landmarks) to verify:\n#    - Data accessibility and integrity\n#    - Feature count and schema correctness\n#    - Presence of feature stamping columns (dsid_*) proving S57Advanced worked\n# \n# This serves as both verification that the entire S-57 conversion pipeline\n# succeeded AND as a demonstration of how to programmatically access converted data.\n# =============================================================================\n\n# --- DEPENDENCIES (Already imported in STEP 2: IMPORTS) ---\n# - GeoPandas: For reading spatial data from the database\n# - Fiona: For layer discovery and format handling\n# - project_root: Set in STEP 2, points to repository root\n\n# =============================================================================\n# SECTION 1: CONFIGURE TEST OUTPUT PATHS\n# =============================================================================\n# Define the location of DeepTest artifacts created in Step 7.\n# DeepTest generates one of two formats depending on backend selection:\n\ntest_output_path = project_root / \"tests\" / \"core__real_data\" / \"test_output\"\n\n# BACKEND CHOICE: Uncomment the format you want to inspect\n# Both are identical in content, just different file formats generated by DeepTest\ntest_file = test_output_path / \"s57_deeptest.sqlite\"       # SpatiaLite format (1h 1m conversion time)\n# test_file = test_output_path / \"s57_deeptest.gpkg\"       # GeoPackage format (4h 9m conversion time)\n\n# RATIONALE FOR COMMENTS:\n# - Both formats contain the same converted S-57 data from the same DeepTest run\n# - SQLite (SpatiaLite) is currently active because it showed faster conversion\n# - GeoPackage is kept as an alternative to show both backends produce identical results\n# - You can uncomment either line to switch between formats\n\nprint(f\"Test output directory: {test_output_path}\")\nprint(f\"Test database file:    {test_file}\")\n\n# =============================================================================\n# SECTION 2: DISCOVER ALL AVAILABLE LAYERS\n# =============================================================================\n# The test database contains multiple layers (S-57 object classes).\n# Fiona provides layer discovery without loading all data into memory.\n\ntry:\n    # List all layers/tables in the spatial database\n    # Returns a list of S-57 object class names (e.g., 'lndmrk', 'lndelv', 'soundg')\n    layer_names = fiona.listlayers(test_file)\n    \n    print(f\"âœ“ Layers found in '{test_file.name}':\")\n    for name in layer_names:\n        print(f\"  - {name}\")\n    \n    # DATA QUALITY CHECK:\n    # The presence of many layers (typically 60-80) indicates successful conversion\n    # of the full S-57 dataset with all object classes represented\n    \nexcept fiona.errors.DriverError as e:\n    # File access error: Database may be corrupted, locked, or inaccessible\n    print(f\"âœ— Error: Could not open the file '{test_file}'.\")\n    print(f\"  Please check that:\")\n    print(f\"    1. The path is correct: {test_file}\")\n    print(f\"    2. DeepTest has completed successfully (workflow_options['run_deeptest'] = True)\")\n    print(f\"    3. The file is not corrupted or locked by another process\")\n    print(f\"  Technical details: {e}\")\n    exit()  # Stop execution if we can't access the test data\n\n# =============================================================================\n# SECTION 3: SELECT AND READ A SPECIFIC LAYER\n# =============================================================================\n# Demonstrates how to programmatically extract and analyze a single S-57 layer.\n# We use the LNDMRK (Landmark) layer as an example.\n\nselected_layer_name = 'lndmrk'  # S-57 Object Class: Landmarks (lights, towers, beacons, etc.)\n\nif selected_layer_name in layer_names:\n    print(f\"\\nâœ“ Reading layer: '{selected_layer_name}'...\")\n    \n    # Load the entire layer into a GeoDataFrame (in-memory analysis)\n    # engine='fiona' ensures consistent handling of S-57 data types across formats\n    gdf = gpd.read_file(test_file, layer=selected_layer_name, engine=\"fiona\")\n    \n    # FEATURE STAMPING VALIDATION:\n    # If S57Advanced conversion succeeded, each feature includes metadata:\n    # - dsid_dsnm: Source ENC name (e.g., 'US1EEZ1M')\n    # - dsid_edtn: ENC edition number (e.g., 9)\n    # - dsid_updn: ENC update number (e.g., 0)\n    # These columns prove the feature came from a specific chart version\n    \n    print(f\"\\nâœ“ Successfully loaded {len(gdf)} features from '{selected_layer_name}'.\")\n    print(f\"  (This landmark layer contains {len(gdf)} nautical features)\")\n    \n    # OPTIONAL DATA FILTERING:\n    # Demonstrate filtering by a specific attribute\n    # colpat = Color Pattern (e.g., red/white striped)\n    test_df = gdf[gdf['colpat'].notna()]\n    print(f\"\\n  Filtered subset: {len(test_df)} features with color pattern information\")\n    \n    # =============================================================================\n    # SECTION 4: INSPECT DATA SCHEMA AND QUALITY\n    # =============================================================================\n    # Display comprehensive schema information to verify data integrity\n    \n    print(f\"\\nâœ“ Layer Information and Schema:\")\n    print(f\"  Total features: {len(gdf)}\")\n    print(f\"  Total attributes: {len(gdf.columns)}\")\n    print(f\"\\n  Column breakdown:\")\n    gdf.info()  # Displays:\n                # - Column names and data types\n                # - Non-null counts (missing data detection)\n                # - Memory usage\n    \n    # CRITICAL VALIDATION CHECKS:\n    # 1. Presence of 'geometry' column confirms geospatial data is intact\n    # 2. Presence of 'dsid_dsnm', 'dsid_edtn', 'dsid_updn' confirms feature stamping\n    # 3. Non-null geometry on all features confirms successful conversion\n    # 4. S-57 standard columns (rcid, objl, catlmk, etc.) indicate proper schema\n    \n    print(f\"\\nâœ“ Data validation passed:\")\n    print(f\"  âœ“ Geometry column present: {('geometry' in gdf.columns)}\")\n    print(f\"  âœ“ Feature stamping present: {all(col in gdf.columns for col in ['dsid_dsnm', 'dsid_edtn', 'dsid_updn'])}\")\n    print(f\"  âœ“ S-57 schema intact: {('objl' in gdf.columns and 'catlmk' in gdf.columns)}\")\n    \nelse:\n    # Layer selection error: The requested layer doesn't exist in the database\n    print(f\"\\nâœ— Error: Layer '{selected_layer_name}' not found in the database.\")\n    print(f\"\\nAvailable layers:\")\n    for name in sorted(layer_names):\n        print(f\"  - {name}\")\n    print(f\"\\nTo inspect a different layer, change 'selected_layer_name' above.\")\n\n# =============================================================================\n# NEXT STEPS\n# =============================================================================\n# Now that you've verified the DeepTest output, you can:\n# 1. Modify 'selected_layer_name' to inspect other S-57 object classes\n# 2. Apply spatial analysis (buffering, intersection queries, etc.)\n# 3. Export subsets to other formats for external tools\n# 4. Validate specific maritime features (e.g., navigable areas, obstructions)\n# =============================================================================",
   "id": "f2ac01d800bde7b4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "uv-maritime",
   "language": "python",
   "display_name": "UV Maritime Environment"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
